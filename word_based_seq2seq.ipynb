{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":349267,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":291641,"modelId":312301}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[]}},"nbformat_minor":0,"nbformat":4,"cells":[{"cell_type":"code","source":["!pip install datasets evaluate"],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:34:00.736062Z","iopub.execute_input":"2025-04-22T05:34:00.736241Z","iopub.status.idle":"2025-04-22T05:34:05.856197Z","shell.execute_reply.started":"2025-04-22T05:34:00.736224Z","shell.execute_reply":"2025-04-22T05:34:05.855533Z"},"id":"4Ijdl7auGX4Q","outputId":"f47e7da0-2cee-42be-d6f2-54551c6060c6"},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: fsspec, evaluate\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.3.2\n    Uninstalling fsspec-2025.3.2:\n      Successfully uninstalled fsspec-2025.3.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.8.4.1 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.3.3.83 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.9.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.3.90 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.8.93 which is incompatible.\ntorch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.8.93 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed evaluate-0.4.3 fsspec-2024.12.0\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":["import gc\n","import torch\n","# Free cached memory and clean up\n","torch.cuda.empty_cache()\n","gc.collect()\n","\n","# Check CUDA availability\n","if torch.cuda.is_available():\n","    total_memory = torch.cuda.get_device_properties(0).total_memory\n","    reserved_memory = torch.cuda.memory_reserved(0)\n","    allocated_memory = torch.cuda.memory_allocated(0)\n","    free_memory = total_memory - reserved_memory\n","    print(f\"Total GPU memory: {total_memory / 1e9:.2f} GB\")\n","    print(f\"Reserved memory: {reserved_memory / 1e9:.2f} GB\")\n","    print(f\"Allocated memory: {allocated_memory / 1e9:.2f} GB\")\n","    print(f\"Free memory: {free_memory / 1e9:.2f} GB\")\n","else:\n","    print(\"CUDA is not available\")"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-21T13:54:07.148123Z","iopub.execute_input":"2025-04-21T13:54:07.148359Z","iopub.status.idle":"2025-04-21T13:54:10.791640Z","shell.execute_reply.started":"2025-04-21T13:54:07.148339Z","shell.execute_reply":"2025-04-21T13:54:10.791054Z"},"id":"xKo9xZ3fGX4S","outputId":"1b31f27c-fa2a-43c1-91e3-b131bcb85773"},"outputs":[{"name":"stdout","text":"Total GPU memory: 15.83 GB\nReserved memory: 0.00 GB\nAllocated memory: 0.00 GB\nFree memory: 15.83 GB\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":["import os\n","import torch\n","os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n","torch.cuda.synchronize()"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:34:16.213416Z","iopub.execute_input":"2025-04-22T05:34:16.213926Z","iopub.status.idle":"2025-04-22T05:34:20.401854Z","shell.execute_reply.started":"2025-04-22T05:34:16.213903Z","shell.execute_reply":"2025-04-22T05:34:20.401079Z"},"id":"nJRhmlgHGX4S"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["#Import library"],"metadata":{"id":"kOopfmRnGh7C"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import random\n","import numpy as np\n","from datasets import load_dataset\n","from tokenizers import Tokenizer\n","from tokenizers.models import WordLevel\n","from tokenizers.trainers import WordLevelTrainer\n","from tokenizers.pre_tokenizers import Whitespace\n","import tqdm\n","import evaluate"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:34:20.403119Z","iopub.execute_input":"2025-04-22T05:34:20.403507Z","iopub.status.idle":"2025-04-22T05:34:42.724914Z","shell.execute_reply.started":"2025-04-22T05:34:20.403482Z","shell.execute_reply":"2025-04-22T05:34:42.724371Z"},"id":"W4Q3Fci0GX4S","outputId":"b682558d-2a14-485e-c1fe-257d340fbf76"},"outputs":[{"name":"stderr","text":"2025-04-22 05:34:26.904115: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1745300067.118812      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1745300067.179926      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":["# Load dataset"],"metadata":{"id":"lbGJ4aiuGjeq"}},{"cell_type":"code","source":["ds = load_dataset(\"thainq107/iwslt2015-en-vi\")\n","train_data, valid_data, test_data = ds[\"train\"], ds[\"validation\"], ds[\"test\"]"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:34:42.725559Z","iopub.execute_input":"2025-04-22T05:34:42.726152Z","iopub.status.idle":"2025-04-22T05:34:45.341868Z","shell.execute_reply.started":"2025-04-22T05:34:42.726122Z","shell.execute_reply":"2025-04-22T05:34:45.341235Z"},"colab":{"referenced_widgets":["a84a1176c3284db98adea7aa12d8dfa7","ae473ec757a84e258604d663cc40809f","e6f05a35b34c4518af25b11286eb3ab0","27ea73c20ff9473c81a5dd1497da8ebe","aa2b59c233cb47dca2e12e4b5fa179c8","56603c82447543689d2c16a4ea9c2694"]},"id":"579vhKO1GX4S","outputId":"d7064d80-1f59-43c4-b48f-d81d99678176"},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/522 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a84a1176c3284db98adea7aa12d8dfa7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/17.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ae473ec757a84e258604d663cc40809f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"validation-00000-of-00001.parquet:   0%|          | 0.00/181k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e6f05a35b34c4518af25b11286eb3ab0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/133317 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27ea73c20ff9473c81a5dd1497da8ebe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/1268 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa2b59c233cb47dca2e12e4b5fa179c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1268 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56603c82447543689d2c16a4ea9c2694"}},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":["# Train word_based tokenizers"],"metadata":{"id":"ZK1Cv8XEGnJp"}},{"cell_type":"code","source":["# English tokenizer\n","tokenizer_en = Tokenizer(WordLevel(unk_token=\"<unk>\"))\n","tokenizer_en.pre_tokenizer = Whitespace()\n","trainer_en = WordLevelTrainer(\n","    vocab_size=30000,\n","    special_tokens=[\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"]\n",")\n","tokenizer_en.train_from_iterator(train_data[\"en\"], trainer=trainer_en)\n","\n","# Vietnamese tokenizer\n","tokenizer_vi = Tokenizer(WordLevel(unk_token=\"<unk>\"))\n","tokenizer_vi.pre_tokenizer = Whitespace()\n","trainer_vi = WordLevelTrainer(\n","    vocab_size=30000,\n","    special_tokens=[\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"]\n",")\n","tokenizer_vi.train_from_iterator(train_data[\"vi\"], trainer=trainer_vi)"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:34:45.343602Z","iopub.execute_input":"2025-04-22T05:34:45.344104Z","iopub.status.idle":"2025-04-22T05:34:48.320715Z","shell.execute_reply.started":"2025-04-22T05:34:45.344076Z","shell.execute_reply":"2025-04-22T05:34:48.319928Z"},"id":"S6KzavmtGX4T"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["# Tokenize & numericalize"],"metadata":{"id":"81d7yjGsGtxL"}},{"cell_type":"code","source":["def tokenize_example(example, tokenizer_src, tokenizer_trg, sos_token, eos_token, max_length=1000):\n","    # Source = English, Target = Vietnamese\n","    src_ids = tokenizer_src.encode(example[\"en\"]).ids[:max_length]\n","    trg_ids = tokenizer_trg.encode(example[\"vi\"]).ids[:max_length]\n","    # Add <sos> and <eos>\n","    src = [tokenizer_src.token_to_id(sos_token)] + src_ids + [tokenizer_src.token_to_id(eos_token)]\n","    trg = [tokenizer_trg.token_to_id(sos_token)] + trg_ids + [tokenizer_trg.token_to_id(eos_token)]\n","    return {\"en_ids\": src, \"vi_ids\": trg}\n","\n","fn_kwargs = {\n","    \"tokenizer_src\": tokenizer_en,\n","    \"tokenizer_trg\": tokenizer_vi,\n","    \"sos_token\": \"<sos>\",\n","    \"eos_token\": \"<eos>\",\n","    \"max_length\": 30\n","}\n","\n","# Note: we no longer remove_columns=[\"en\",\"vi\"]\n","train_data = train_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n","valid_data = valid_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n","test_data  = test_data.map(tokenize_example, fn_kwargs=fn_kwargs)"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:34:48.321566Z","iopub.execute_input":"2025-04-22T05:34:48.321782Z","iopub.status.idle":"2025-04-22T05:35:12.792167Z","shell.execute_reply.started":"2025-04-22T05:34:48.321765Z","shell.execute_reply":"2025-04-22T05:35:12.791374Z"},"colab":{"referenced_widgets":["88ac699612a14a8ca9ce499a9bf10119","a0a15b64febc4bde9ea9801a6d0b63a6","dc6643745d3647428e9ff202a6aaa2da"]},"id":"gYOoKUxHGX4T","outputId":"d50709f3-ba2b-4090-9dca-ec28d2f4a979"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/133317 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88ac699612a14a8ca9ce499a9bf10119"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1268 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0a15b64febc4bde9ea9801a6d0b63a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1268 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc6643745d3647428e9ff202a6aaa2da"}},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":["# Convert lists to torch.Tensor"],"metadata":{"id":"h5gL1pu3GvS4"}},{"cell_type":"code","source":["def to_tensor(example):\n","    return {\n","        \"en_ids\": torch.tensor(example[\"en_ids\"], dtype=torch.long),\n","        \"vi_ids\": torch.tensor(example[\"vi_ids\"], dtype=torch.long)\n","    }\n","\n","# after tokenization (lists of ints in en_ids/vi_ids):\n","train_data = train_data.with_format(\n","    type=\"torch\",\n","    columns=[\"en_ids\",\"vi_ids\"],\n","    output_all_columns=True\n",")\n","valid_data = valid_data.with_format(type=\"torch\", columns=[\"en_ids\",\"vi_ids\"], output_all_columns=True)\n","test_data  = test_data.with_format(type=\"torch\", columns=[\"en_ids\",\"vi_ids\"], output_all_columns=True)"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:35:12.793196Z","iopub.execute_input":"2025-04-22T05:35:12.793698Z","iopub.status.idle":"2025-04-22T05:35:12.801564Z","shell.execute_reply.started":"2025-04-22T05:35:12.793672Z","shell.execute_reply":"2025-04-22T05:35:12.800733Z"},"id":"GCDKZflfGX4T"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["# DataLoader setup"],"metadata":{"id":"nZjJPypjGzs7"}},{"cell_type":"code","source":["def get_collate_fn(pad_id):\n","    def collate_fn(batch):\n","        src = [ex[\"en_ids\"] for ex in batch]\n","        trg = [ex[\"vi_ids\"] for ex in batch]\n","        src = nn.utils.rnn.pad_sequence(src, padding_value=pad_id)\n","        trg = nn.utils.rnn.pad_sequence(trg, padding_value=pad_id)\n","        return {\"en_ids\": src, \"vi_ids\": trg}\n","    return collate_fn\n","\n","pad_id = tokenizer_en.token_to_id(\"<pad>\")\n","batch_size = 32\n","\n","train_loader = torch.utils.data.DataLoader(\n","    train_data, batch_size=batch_size,\n","    collate_fn=get_collate_fn(pad_id), shuffle=True\n",")\n","valid_loader = torch.utils.data.DataLoader(\n","    valid_data, batch_size=batch_size,\n","    collate_fn=get_collate_fn(pad_id)\n",")\n","test_loader  = torch.utils.data.DataLoader(\n","    test_data,  batch_size=batch_size,\n","    collate_fn=get_collate_fn(pad_id)\n",")"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:35:12.802341Z","iopub.execute_input":"2025-04-22T05:35:12.802657Z","iopub.status.idle":"2025-04-22T05:35:13.131202Z","shell.execute_reply.started":"2025-04-22T05:35:12.802633Z","shell.execute_reply":"2025-04-22T05:35:13.130453Z"},"id":"Iz-UZXuDGX4T"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["# Seq2seq model"],"metadata":{"id":"fFIL7iYqG1Gp"}},{"cell_type":"code","source":["class Encoder(nn.Module):\n","    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n","        super().__init__()\n","        self.embedding = nn.Embedding(input_dim, emb_dim)\n","        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, src):\n","        # src = [src_len, batch_size]\n","        embedded = self.dropout(self.embedding(src))\n","        outputs, (hidden, cell) = self.rnn(embedded)\n","        return hidden, cell\n","\n","class Decoder(nn.Module):\n","    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n","        super().__init__()\n","        self.embedding = nn.Embedding(output_dim, emb_dim)\n","        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n","        self.fc_out = nn.Linear(hid_dim, output_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, input, hidden, cell):\n","        # input = [batch_size]\n","        input = input.unsqueeze(0)\n","        # input = [1, batch_size]\n","        embedded = self.dropout(self.embedding(input))\n","        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n","        # output = [1, batch_size, hid_dim]\n","        prediction = self.fc_out(output.squeeze(0))\n","        # prediction = [batch_size, output_dim]\n","        return prediction, hidden, cell\n","\n","class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder, device):\n","        super().__init__()\n","        assert encoder.rnn.hidden_size == decoder.rnn.hidden_size\n","        assert encoder.rnn.num_layers == decoder.rnn.num_layers\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.device = device\n","\n","    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n","        # src = [src_len, batch_size], trg = [trg_len, batch_size]\n","        batch_size = trg.shape[1]\n","        trg_len = trg.shape[0]\n","        vocab_size = self.decoder.fc_out.out_features\n","        outputs = torch.zeros(trg_len, batch_size, vocab_size).to(self.device)\n","\n","        hidden, cell = self.encoder(src)\n","        input = trg[0, :]  # <sos>\n","\n","        for t in range(1, trg_len):\n","            output, hidden, cell = self.decoder(input, hidden, cell)\n","            outputs[t] = output\n","            teacher_force = random.random() < teacher_forcing_ratio\n","            top1 = output.argmax(1)\n","            input = trg[t] if teacher_force else top1\n","\n","        return outputs\n"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:35:13.131979Z","iopub.execute_input":"2025-04-22T05:35:13.132277Z","iopub.status.idle":"2025-04-22T05:35:13.146670Z","shell.execute_reply.started":"2025-04-22T05:35:13.132253Z","shell.execute_reply":"2025-04-22T05:35:13.146127Z"},"id":"OOyR_Yv9GX4T"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["#Initialize Model + Training Setup"],"metadata":{"id":"KXFvuGfbG3GJ"}},{"cell_type":"code","source":["INPUT_DIM  = tokenizer_en.get_vocab_size()\n","OUTPUT_DIM = tokenizer_vi.get_vocab_size()\n","ENC_EMB_DIM = 256\n","DEC_EMB_DIM = 256\n","HID_DIM     = 512\n","N_LAYERS    = 2\n","ENC_DROPOUT = 0.5\n","DEC_DROPOUT = 0.5\n","DEVICE      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n","dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n","model = Seq2Seq(enc, dec, DEVICE).to(DEVICE)\n","\n","def init_weights(m):\n","    for p in m.parameters():\n","        nn.init.uniform_(p.data, -0.08, 0.08)\n","model.apply(init_weights)\n","\n","optimizer = optim.Adam(model.parameters())\n","criterion = nn.CrossEntropyLoss(ignore_index=pad_id)\n","\n","CHECKPOINT_PATH = '/kaggle/input/seqseq/pytorch/default/1/best-model.pt'\n","\n","if os.path.isfile(CHECKPOINT_PATH):\n","    model.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=DEVICE))\n","    print(f\"✔ Loaded checkpoint from {CHECKPOINT_PATH}, resuming training.\")\n","else:\n","    # only initialize weights if there's no checkpoint\n","    def init_weights(m):\n","        for p in m.parameters():\n","            nn.init.uniform_(p.data, -0.08, 0.08)\n","    model.apply(init_weights)\n","    print(\"✗ No checkpoint found — training from scratch.\")\n","\n","print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:35:13.147437Z","iopub.execute_input":"2025-04-22T05:35:13.147665Z","iopub.status.idle":"2025-04-22T05:35:15.069749Z","shell.execute_reply.started":"2025-04-22T05:35:13.147640Z","shell.execute_reply":"2025-04-22T05:35:15.069096Z"},"id":"EjD4ky7wGX4T","outputId":"852ee8c7-f287-473f-e025-6afe30b756ac"},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/790604878.py:26: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=DEVICE))\n","output_type":"stream"},{"name":"stdout","text":"✔ Loaded checkpoint from /kaggle/input/seqseq/pytorch/default/1/best-model.pt, resuming training.\nTrainable parameters: 32,745,717\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":["#Training and Evaluation Functions"],"metadata":{"id":"f9fBvoyqG6fY"}},{"cell_type":"code","source":["def train_fn(model, loader, optimizer, criterion, clip, device):\n","    model.train()\n","    epoch_loss = 0\n","    for batch in loader:\n","        src = batch[\"en_ids\"].to(device)\n","        trg = batch[\"vi_ids\"].to(device)\n","        optimizer.zero_grad()\n","        output = model(src, trg, teacher_forcing_ratio=0.5)\n","        # output = [trg_len, batch_size, vocab_size]\n","        output_dim = output.shape[-1]\n","        out = output[1:].view(-1, output_dim)\n","        tgt = trg[1:].view(-1)\n","        loss = criterion(out, tgt)\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n","        optimizer.step()\n","        epoch_loss += loss.item()\n","    return epoch_loss / len(loader)\n","\n","def eval_fn(model, loader, criterion, device):\n","    model.eval()\n","    epoch_loss = 0\n","    with torch.no_grad():\n","        for batch in loader:\n","            src = batch[\"en_ids\"].to(device)\n","            trg = batch[\"vi_ids\"].to(device)\n","            output = model(src, trg, teacher_forcing_ratio=0.5)  # no teacher forcing\n","            output_dim = output.shape[-1]\n","            out = output[1:].view(-1, output_dim)\n","            tgt = trg[1:].view(-1)\n","            loss = criterion(out, tgt)\n","            epoch_loss += loss.item()\n","    return epoch_loss / len(loader)\n"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:35:15.071490Z","iopub.execute_input":"2025-04-22T05:35:15.071710Z","iopub.status.idle":"2025-04-22T05:35:15.078655Z","shell.execute_reply.started":"2025-04-22T05:35:15.071693Z","shell.execute_reply":"2025-04-22T05:35:15.077778Z"},"id":"4yNwbr7yGX4U"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":["# Testing + BLEU"],"metadata":{"id":"YhQmjjE2G9NP"}},{"cell_type":"code","source":["N_EPOCHS = 1\n","CLIP     = 1.0\n","\n","best_valid = float('inf')\n","for epoch in range(N_EPOCHS):\n","    train_loss = train_fn(model, train_loader, optimizer, criterion, CLIP, DEVICE)\n","    valid_loss = eval_fn(model, valid_loader, criterion, DEVICE)\n","    if valid_loss < best_valid:\n","        best_valid = valid_loss\n","        torch.save(model.state_dict(), 'best-model.pt')\n","    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.3f} | Train PPL: {np.exp(train_loss):.3f}\")\n","    print(f\"          | Val   Loss: {valid_loss:.3f} | Val   PPL: {np.exp(valid_loss):.3f}\")"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:35:15.079430Z","iopub.execute_input":"2025-04-22T05:35:15.079644Z","iopub.status.idle":"2025-04-22T05:53:01.036981Z","shell.execute_reply.started":"2025-04-22T05:35:15.079630Z","shell.execute_reply":"2025-04-22T05:53:01.036310Z"},"id":"E8114kpwGX4U","outputId":"a61e727c-d19d-4f88-f550-3ff9565acd0d"},"outputs":[{"name":"stdout","text":"Epoch 1 | Train Loss: 3.602 | Train PPL: 36.681\n          | Val   Loss: 4.005 | Val   PPL: 54.879\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":["# test model"],"metadata":{"id":"xe5T1_CjG_kz"}},{"cell_type":"code","source":["model.load_state_dict(torch.load('best-model.pt'))\n","test_loss = eval_fn(model, test_loader, criterion, DEVICE)\n","print(f\"Test Loss: {test_loss:.3f} | Test PPL: {np.exp(test_loss):.3f}\")\n","\n","# Translation helper\n","def translate_sentence(\n","    sentence, model, tokenizer_src, tokenizer_trg,\n","    lower=True, sos_token=\"<sos>\", eos_token=\"<eos>\",\n","    device=DEVICE, max_len=30\n","):\n","    model.eval()\n","    tokens = sentence.split()  # already whitespace-tokenized\n","    tokens = [sos_token] + tokens + [eos_token]\n","    src_ids = tokenizer_src.encode(\" \".join(tokens)).ids\n","    src_tensor = torch.LongTensor(src_ids).unsqueeze(1).to(device)\n","    hidden, cell = model.encoder(src_tensor)\n","\n","    outputs = [tokenizer_trg.token_to_id(sos_token)]\n","    for _ in range(max_len):\n","        prev = torch.LongTensor([outputs[-1]]).to(device)\n","        pred, hidden, cell = model.decoder(prev, hidden, cell)\n","        top1 = pred.argmax(1).item()\n","        outputs.append(top1)\n","        if top1 == tokenizer_trg.token_to_id(eos_token):\n","            break\n","\n","    return tokenizer_trg.decode(outputs)\n","\n","# Compute BLEU on test set\n","bleu = evaluate.load(\"bleu\")\n","predictions = []\n","references  = []\n","\n","for ex in tqdm.tqdm(test_data):\n","    # ex[\"en\"] & ex[\"vi\"] are still present because we didn't remove them\n","    pred = translate_sentence(\n","        ex[\"en\"], model,\n","        tokenizer_en, tokenizer_vi,\n","        device=DEVICE\n","    )\n","    predictions.append(pred)\n","    references.append([ex[\"vi\"]])\n","\n","results = bleu.compute(predictions=predictions, references=references)\n","print(f\"BLEU score = {results['bleu']:.4f}\")"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:53:01.038643Z","iopub.execute_input":"2025-04-22T05:53:01.039229Z","iopub.status.idle":"2025-04-22T05:53:31.831742Z","shell.execute_reply.started":"2025-04-22T05:53:01.039207Z","shell.execute_reply":"2025-04-22T05:53:31.830759Z"},"colab":{"referenced_widgets":["4ffea5d3e55448ba8654606fc003aec4","2285e5cb001c4b489afd0cf5b87a0158","53c25967df9e4cbf9ff731bb85902289"]},"id":"_eiYE7WGGX4U","outputId":"6e8bb14f-a0e2-41e8-c1f5-b431a7371187"},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_31/1755102884.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  model.load_state_dict(torch.load('best-model.pt'))\n","output_type":"stream"},{"name":"stdout","text":"Test Loss: 4.000 | Test PPL: 54.617\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ffea5d3e55448ba8654606fc003aec4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2285e5cb001c4b489afd0cf5b87a0158"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53c25967df9e4cbf9ff731bb85902289"}},"metadata":{}},{"name":"stderr","text":"100%|██████████| 1268/1268 [00:24<00:00, 50.87it/s]\n","output_type":"stream"},{"name":"stdout","text":"BLEU score = 0.0619\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":["sentence = test_data[0][\"en\"]\n","expected_translation = test_data[0][\"vi\"]\n","print(\"Source (English):\", sentence)\n","print(\"Expected Translation (Vietnamese):\", expected_translation)\n","translation = translate_sentence(sentence, model, tokenizer_en, tokenizer_vi,device=DEVICE)\n","print(\"Model Translation:\", translation)"],"metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-22T05:53:31.832535Z","iopub.execute_input":"2025-04-22T05:53:31.832780Z","iopub.status.idle":"2025-04-22T05:53:31.870168Z","shell.execute_reply.started":"2025-04-22T05:53:31.832761Z","shell.execute_reply":"2025-04-22T05:53:31.869604Z"},"id":"QmDE3-TkGX4V","outputId":"61034118-1f75-4999-b5a0-fc3990482ec3"},"outputs":[{"name":"stdout","text":"Source (English): When I was little , I thought my country was the best on the planet , and I grew up singing a song called &quot; Nothing To Envy . &quot;\nExpected Translation (Vietnamese): Khi tôi còn nhỏ , Tôi nghĩ rằng BắcTriều Tiên là đất nước tốt nhất trên thế giới và tôi thường hát bài &quot; Chúng ta chẳng có gì phải ghen tị . &quot;\nModel Translation: Khi tôi lớn , tôi nghĩ là đất nước của tôi sống , và và tôi tôi tôi tôi gọi là & quot ; Tôi & quot ; Tôi đang\n","output_type":"stream"}],"execution_count":null}]}