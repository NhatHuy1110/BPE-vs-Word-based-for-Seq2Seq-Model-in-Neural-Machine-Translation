{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-21T13:25:07.562960Z",
     "iopub.status.busy": "2025-04-21T13:25:07.562190Z",
     "iopub.status.idle": "2025-04-21T13:25:12.652605Z",
     "shell.execute_reply": "2025-04-21T13:25:12.651810Z",
     "shell.execute_reply.started": "2025-04-21T13:25:07.562930Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.30.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\n",
      "Collecting fsspec>=2021.05.0 (from fsspec[http]>=2021.05.0->evaluate)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.16)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.1)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\n",
      "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fsspec, evaluate\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.2\n",
      "    Uninstalling fsspec-2025.3.2:\n",
      "      Successfully uninstalled fsspec-2025.3.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.8.4.1 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.3.3.83 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.9.90 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.7.3.90 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.8.93 which is incompatible.\n",
      "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.8.93 which is incompatible.\n",
      "bigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed evaluate-0.4.3 fsspec-2024.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T13:25:12.654665Z",
     "iopub.status.busy": "2025-04-21T13:25:12.654170Z",
     "iopub.status.idle": "2025-04-21T13:25:33.558042Z",
     "shell.execute_reply": "2025-04-21T13:25:33.557174Z",
     "shell.execute_reply.started": "2025-04-21T13:25:12.654611Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-21 13:25:17.448567: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745241917.652473      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745241917.711031      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "import tqdm\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T13:25:33.559697Z",
     "iopub.status.busy": "2025-04-21T13:25:33.559003Z",
     "iopub.status.idle": "2025-04-21T13:25:35.445228Z",
     "shell.execute_reply": "2025-04-21T13:25:35.444411Z",
     "shell.execute_reply.started": "2025-04-21T13:25:33.559665Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60bd60be8a154094a59934c247a86f72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/522 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8c904ebba6d4afabfc350ec1816cda7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/17.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c77bb46205e4409a9d4904b4c27c1a14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation-00000-of-00001.parquet:   0%|          | 0.00/181k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51820ed36bd84830b73ba473745c38be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/133317 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a10c6db60bf4f55a4f13cc570216e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/1268 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2260b8678e1844f583cc5271ecbb4646",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1268 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset(\"thainq107/iwslt2015-en-vi\")\n",
    "train_data, valid_data, test_data = ds[\"train\"], ds[\"validation\"], ds[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T13:25:35.447003Z",
     "iopub.status.busy": "2025-04-21T13:25:35.446733Z",
     "iopub.status.idle": "2025-04-21T13:25:35.452386Z",
     "shell.execute_reply": "2025-04-21T13:25:35.451668Z",
     "shell.execute_reply.started": "2025-04-21T13:25:35.446985Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['en', 'vi'],\n",
       "        num_rows: 133317\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['en', 'vi'],\n",
       "        num_rows: 1268\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['en', 'vi'],\n",
       "        num_rows: 1268\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train BPE tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T13:25:54.348503Z",
     "iopub.status.busy": "2025-04-21T13:25:54.348202Z",
     "iopub.status.idle": "2025-04-21T13:25:59.709849Z",
     "shell.execute_reply": "2025-04-21T13:25:59.708961Z",
     "shell.execute_reply.started": "2025-04-21T13:25:54.348474Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# English tokenizer\n",
    "tokenizer_en = Tokenizer(BPE())\n",
    "tokenizer_en.pre_tokenizer = Whitespace()\n",
    "trainer_en = BpeTrainer(\n",
    "    vocab_size=30000,\n",
    "    special_tokens=[\"<unk>\", \"<p ad>\", \"<sos>\", \"<eos>\"]\n",
    ")\n",
    "tokenizer_en.train_from_iterator(train_data[\"en\"], trainer=trainer_en)\n",
    "\n",
    "# Vietnamese tokenizer\n",
    "tokenizer_vi = Tokenizer(BPE())\n",
    "tokenizer_vi.pre_tokenizer = Whitespace()\n",
    "trainer_vi = BpeTrainer(\n",
    "    vocab_size=30000,\n",
    "    special_tokens=[\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"]\n",
    ")\n",
    "tokenizer_vi.train_from_iterator(train_data[\"vi\"], trainer=trainer_vi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize & numericalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T13:25:59.711475Z",
     "iopub.status.busy": "2025-04-21T13:25:59.711255Z",
     "iopub.status.idle": "2025-04-21T13:26:29.463325Z",
     "shell.execute_reply": "2025-04-21T13:26:29.462557Z",
     "shell.execute_reply.started": "2025-04-21T13:25:59.711459Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c6f1ced97a04d8ab66d1107c51a8503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/133317 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "133b7e13de5d4d16bd5f6ada5a11b52d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1268 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64ab8d708ff04c1ea66292c569e5f0c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1268 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_example(example, tokenizer_src, tokenizer_trg, sos_token, eos_token, max_length=1000):\n",
    "    # Source = English, Target = Vietnamese\n",
    "    src_ids = tokenizer_src.encode(example[\"en\"]).ids[:max_length]\n",
    "    trg_ids = tokenizer_trg.encode(example[\"vi\"]).ids[:max_length]\n",
    "    # Add <sos> and <eos>\n",
    "    src = [tokenizer_src.token_to_id(sos_token)] + src_ids + [tokenizer_src.token_to_id(eos_token)]\n",
    "    trg = [tokenizer_trg.token_to_id(sos_token)] + trg_ids + [tokenizer_trg.token_to_id(eos_token)]\n",
    "    return {\"en_ids\": src, \"vi_ids\": trg}\n",
    "\n",
    "fn_kwargs = {\n",
    "    \"tokenizer_src\": tokenizer_en,\n",
    "    \"tokenizer_trg\": tokenizer_vi,\n",
    "    \"sos_token\": \"<sos>\",\n",
    "    \"eos_token\": \"<eos>\",\n",
    "    \"max_length\": 30\n",
    "}\n",
    "\n",
    "# Note: we no longer remove_columns=[\"en\",\"vi\"]\n",
    "train_data = train_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "valid_data = valid_data.map(tokenize_example, fn_kwargs=fn_kwargs)\n",
    "test_data  = test_data.map(tokenize_example, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert lists to torch.Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T13:26:29.464878Z",
     "iopub.status.busy": "2025-04-21T13:26:29.464105Z",
     "iopub.status.idle": "2025-04-21T13:26:29.472611Z",
     "shell.execute_reply": "2025-04-21T13:26:29.471895Z",
     "shell.execute_reply.started": "2025-04-21T13:26:29.464852Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def to_tensor(example):\n",
    "    return {\n",
    "        \"en_ids\": torch.tensor(example[\"en_ids\"], dtype=torch.long),\n",
    "        \"vi_ids\": torch.tensor(example[\"vi_ids\"], dtype=torch.long)\n",
    "    }\n",
    "\n",
    "# after tokenization (lists of ints in en_ids/vi_ids):\n",
    "train_data = train_data.with_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"en_ids\",\"vi_ids\"],\n",
    "    output_all_columns=True\n",
    ")\n",
    "valid_data = valid_data.with_format(type=\"torch\", columns=[\"en_ids\",\"vi_ids\"], output_all_columns=True)\n",
    "test_data  = test_data.with_format(type=\"torch\", columns=[\"en_ids\",\"vi_ids\"], output_all_columns=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T13:26:29.474961Z",
     "iopub.status.busy": "2025-04-21T13:26:29.474655Z",
     "iopub.status.idle": "2025-04-21T13:26:29.753688Z",
     "shell.execute_reply": "2025-04-21T13:26:29.752998Z",
     "shell.execute_reply.started": "2025-04-21T13:26:29.474930Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_collate_fn(pad_id):\n",
    "    def collate_fn(batch):\n",
    "        src = [ex[\"en_ids\"] for ex in batch]\n",
    "        trg = [ex[\"vi_ids\"] for ex in batch]\n",
    "        src = nn.utils.rnn.pad_sequence(src, padding_value=pad_id)\n",
    "        trg = nn.utils.rnn.pad_sequence(trg, padding_value=pad_id)\n",
    "        return {\"en_ids\": src, \"vi_ids\": trg}\n",
    "    return collate_fn\n",
    "\n",
    "pad_id = tokenizer_en.token_to_id(\"<pad>\")\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, batch_size=batch_size,\n",
    "    collate_fn=get_collate_fn(pad_id), shuffle=True\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_data, batch_size=batch_size,\n",
    "    collate_fn=get_collate_fn(pad_id)\n",
    ")\n",
    "test_loader  = torch.utils.data.DataLoader(\n",
    "    test_data,  batch_size=batch_size,\n",
    "    collate_fn=get_collate_fn(pad_id)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T13:26:29.754754Z",
     "iopub.status.busy": "2025-04-21T13:26:29.754544Z",
     "iopub.status.idle": "2025-04-21T13:26:29.767207Z",
     "shell.execute_reply": "2025-04-21T13:26:29.766452Z",
     "shell.execute_reply.started": "2025-04-21T13:26:29.754738Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src = [src_len, batch_size]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        # input = [batch_size]\n",
    "        input = input.unsqueeze(0)\n",
    "        # input = [1, batch_size]\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        # output = [1, batch_size, hid_dim]\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        # prediction = [batch_size, output_dim]\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        assert encoder.rnn.hidden_size == decoder.rnn.hidden_size\n",
    "        assert encoder.rnn.num_layers == decoder.rnn.num_layers\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # src = [src_len, batch_size], trg = [trg_len, batch_size]\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        vocab_size = self.decoder.fc_out.out_features\n",
    "        outputs = torch.zeros(trg_len, batch_size, vocab_size).to(self.device)\n",
    "\n",
    "        hidden, cell = self.encoder(src)\n",
    "        input = trg[0, :]  # <sos>\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[t] if teacher_force else top1\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Model + Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T13:57:29.104504Z",
     "iopub.status.busy": "2025-04-21T13:57:29.104236Z",
     "iopub.status.idle": "2025-04-21T13:57:29.692149Z",
     "shell.execute_reply": "2025-04-21T13:57:29.691281Z",
     "shell.execute_reply.started": "2025-04-21T13:57:29.104475Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Loaded checkpoint from /kaggle/input/bpe_seq2seq/pytorch/default/1/best-model.pt, resuming training.\n",
      "Trainable parameters: 38,106,416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/884584735.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=DEVICE))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# right:\n",
    "INPUT_DIM  = tokenizer_en.get_vocab_size()\n",
    "OUTPUT_DIM = tokenizer_vi.get_vocab_size()\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM     = 512\n",
    "N_LAYERS    = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "DEVICE      = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "model = Seq2Seq(enc, dec, DEVICE).to(DEVICE)\n",
    "\n",
    "def init_weights(m):\n",
    "    for p in m.parameters():\n",
    "        nn.init.uniform_(p.data, -0.08, 0.08)\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "\n",
    "CHECKPOINT_PATH = '/kaggle/input/bpe_seq2seq/pytorch/default/1/best-model.pt'  \n",
    "\n",
    "if os.path.isfile(CHECKPOINT_PATH):\n",
    "    model.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=DEVICE))\n",
    "    print(f\"✔ Loaded checkpoint from {CHECKPOINT_PATH}, resuming training.\")\n",
    "else:\n",
    "    # only initialize weights if there's no checkpoint\n",
    "    def init_weights(m):\n",
    "        for p in m.parameters():\n",
    "            nn.init.uniform_(p.data, -0.08, 0.08)\n",
    "    model.apply(init_weights)\n",
    "    print(\"✗ No checkpoint found — training from scratch.\")\n",
    "\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T13:57:34.925880Z",
     "iopub.status.busy": "2025-04-21T13:57:34.925411Z",
     "iopub.status.idle": "2025-04-21T13:57:34.933096Z",
     "shell.execute_reply": "2025-04-21T13:57:34.932248Z",
     "shell.execute_reply.started": "2025-04-21T13:57:34.925856Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_fn(model, loader, optimizer, criterion, clip, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in loader:\n",
    "        src = batch[\"en_ids\"].to(device)\n",
    "        trg = batch[\"vi_ids\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg, teacher_forcing_ratio=0.5)\n",
    "        # output = [trg_len, batch_size, vocab_size]\n",
    "        output_dim = output.shape[-1]\n",
    "        out = output[1:].view(-1, output_dim)\n",
    "        tgt = trg[1:].view(-1)\n",
    "        loss = criterion(out, tgt)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(loader)\n",
    "\n",
    "def eval_fn(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            src = batch[\"en_ids\"].to(device)\n",
    "            trg = batch[\"vi_ids\"].to(device)\n",
    "            output = model(src, trg, teacher_forcing_ratio=0.5)  # no teacher forcing\n",
    "            output_dim = output.shape[-1]\n",
    "            out = output[1:].view(-1, output_dim)\n",
    "            tgt = trg[1:].view(-1)\n",
    "            loss = criterion(out, tgt)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T13:57:36.821334Z",
     "iopub.status.busy": "2025-04-21T13:57:36.821044Z",
     "iopub.status.idle": "2025-04-21T14:32:14.452481Z",
     "shell.execute_reply": "2025-04-21T14:32:14.451877Z",
     "shell.execute_reply.started": "2025-04-21T13:57:36.821313Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Train Loss: 3.273 | Train PPL: 26.382\n",
      "          | Val   Loss: 4.087 | Val   PPL: 59.551\n",
      "Epoch 2 | Train Loss: 3.245 | Train PPL: 25.669\n",
      "          | Val   Loss: 4.108 | Val   PPL: 60.810\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 2\n",
    "CLIP     = 1.0\n",
    "\n",
    "best_valid = float('inf')\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train_fn(model, train_loader, optimizer, criterion, CLIP, DEVICE)\n",
    "    valid_loss = eval_fn(model, valid_loader, criterion, DEVICE)\n",
    "    if valid_loss < best_valid:\n",
    "        best_valid = valid_loss\n",
    "        torch.save(model.state_dict(), 'best-model.pt')\n",
    "    print(f\"Epoch {epoch+1} | Train Loss: {train_loss:.3f} | Train PPL: {np.exp(train_loss):.3f}\")\n",
    "    print(f\"          | Val   Loss: {valid_loss:.3f} | Val   PPL: {np.exp(valid_loss):.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing + BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T14:32:14.454012Z",
     "iopub.status.busy": "2025-04-21T14:32:14.453808Z",
     "iopub.status.idle": "2025-04-21T14:32:19.345718Z",
     "shell.execute_reply": "2025-04-21T14:32:19.345019Z",
     "shell.execute_reply.started": "2025-04-21T14:32:14.453996Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31/717956662.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('best-model.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 4.036 | Test PPL: 56.574\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('best-model.pt'))\n",
    "test_loss = eval_fn(model, test_loader, criterion, DEVICE)\n",
    "print(f\"Test Loss: {test_loss:.3f} | Test PPL: {np.exp(test_loss):.3f}\")\n",
    "\n",
    "# Translation helper\n",
    "def translate_sentence(\n",
    "    sentence, model, tokenizer_src, tokenizer_trg,\n",
    "    lower=True, sos_token=\"<sos>\", eos_token=\"<eos>\",\n",
    "    device=DEVICE, max_len=30\n",
    "):\n",
    "    model.eval()\n",
    "    tokens = sentence.split()  # already whitespace-tokenized\n",
    "    tokens = [sos_token] + tokens + [eos_token]\n",
    "    src_ids = tokenizer_src.encode(\" \".join(tokens)).ids\n",
    "    src_tensor = torch.LongTensor(src_ids).unsqueeze(1).to(device)\n",
    "    hidden, cell = model.encoder(src_tensor)\n",
    "\n",
    "    outputs = [tokenizer_trg.token_to_id(sos_token)]\n",
    "    for _ in range(max_len):\n",
    "        prev = torch.LongTensor([outputs[-1]]).to(device)\n",
    "        pred, hidden, cell = model.decoder(prev, hidden, cell)\n",
    "        top1 = pred.argmax(1).item()\n",
    "        outputs.append(top1)\n",
    "        if top1 == tokenizer_trg.token_to_id(eos_token):\n",
    "            break\n",
    "\n",
    "    return tokenizer_trg.decode(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T14:32:19.346493Z",
     "iopub.status.busy": "2025-04-21T14:32:19.346266Z",
     "iopub.status.idle": "2025-04-21T14:32:41.164797Z",
     "shell.execute_reply": "2025-04-21T14:32:41.164048Z",
     "shell.execute_reply.started": "2025-04-21T14:32:19.346475Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0275f3ec6b2d44d6b8f78100fcdf6d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ab8c531bc0c498b97b3f4ef05f046b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e791b78b0bf42818bec003ea05b2025",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1268/1268 [00:20<00:00, 61.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU score = 0.0685\n"
     ]
    }
   ],
   "source": [
    "# Compute BLEU on test set\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "predictions = []\n",
    "references  = []\n",
    "\n",
    "for ex in tqdm.tqdm(test_data):\n",
    "    # ex[\"en\"] & ex[\"vi\"] are still present because we didn't remove them\n",
    "    pred = translate_sentence(\n",
    "        ex[\"en\"], model,\n",
    "        tokenizer_en, tokenizer_vi,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    predictions.append(pred)\n",
    "    references.append([ex[\"vi\"]])\n",
    "\n",
    "results = bleu.compute(predictions=predictions, references=references)\n",
    "print(f\"BLEU score = {results['bleu']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-21T14:32:41.166437Z",
     "iopub.status.busy": "2025-04-21T14:32:41.166166Z",
     "iopub.status.idle": "2025-04-21T14:32:41.197882Z",
     "shell.execute_reply": "2025-04-21T14:32:41.197251Z",
     "shell.execute_reply.started": "2025-04-21T14:32:41.166418Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source (English): When I was little , I thought my country was the best on the planet , and I grew up singing a song called &quot; Nothing To Envy . &quot;\n",
      "Expected Translation (Vietnamese): Khi tôi còn nhỏ , Tôi nghĩ rằng BắcTriều Tiên là đất nước tốt nhất trên thế giới và tôi thường hát bài &quot; Chúng ta chẳng có gì phải ghen tị . &quot;\n",
      "Model Translation: Khi tôi nhỏ , tôi nghĩ rằng tôi là đất nước là một thế giới tốt đẹp nhất và tôi đã hát rằng tôi gọi là & quot ; Ain\n"
     ]
    }
   ],
   "source": [
    "sentence = test_data[0][\"en\"]\n",
    "expected_translation = test_data[0][\"vi\"]\n",
    "print(\"Source (English):\", sentence)\n",
    "print(\"Expected Translation (Vietnamese):\", expected_translation)\n",
    "translation = translate_sentence(sentence, model, tokenizer_en, tokenizer_vi,device=DEVICE)\n",
    "print(\"Model Translation:\", translation)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 312718,
     "modelInstanceId": 292065,
     "sourceId": 349737,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
